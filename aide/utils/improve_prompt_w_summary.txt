Your job is to IMPROVE a previously developed PyGlove + PyTorch NAS solution by **analyzing prior trials** and **refining the search space** to focus on what still matters. Then **fill in the given Python Code Template** (unchanged structure/signatures) with the refined PyGlove search space and Torch implementation to further boost prediction performance.

########### STRICT RULES ###########
- Do NOT change class/method names, signatures, or the outer NAS loop/timeout helper.
- Do NOT add any new print/logging statements anywhere (including during CSV saving).
- Use **PyTorch** for all neural networks and run on GPU if available (`DEVICE` is already set).
- Use **PyGlove** for all search knobs. Do **NOT** pass `default` to `pg.oneof`. Every knob must be executable immediately (i.e., concrete candidate values).
- Script must complete within ~30 minutes on a single GPU.

########### INPUTS YOU CAN RELY ON ###########
- The **Task description**.
- The **previously developed solution** (search space and code skeleton).
- A list of **Model performance** for many trials: for each trial, the chosen **Tested parameters settings** (architecture + hyperparameters) and its **metric** (Validation score).
- **Is the higher the better**: True, if the metric is the higher the better; Otherwise, False.
- **Failure Experience**: all previous experiment summary underperformed its parent solution.
- The **Python Code Template** (provided separately and must remain structurally intact).

########### OBJECTIVES ###########
1) **Use "Failure Experience"** to learn from mistakes and help this improvement. 
   - Diagnosis why it failed: analyze the most plausible cause(s) of degradation (e.g., over-constrained search space, harmful regularization, data leakage fix reducing apparent gains, unstable optimizer region, validation split too small, under/overfitting signals) and any confounders (data sampling, seed sensitivity, feature interactions, compute limits, timeouts).
   - Targeted corrections on what to change next: Consider to update
      - search space: precisely undo harmful constraints; widen or shift ranges only where evidence indicates; freeze knobs shown to be neutral/negative; etc.
      - optimization: adjust learning rate / batch size / schedulers; cap grad-norm; revisit early stopping patience; etc.
      - model/feature: add/remove specific features or heads; revise dimensionalities (e.g., SVD k), activation/dropout; reconcile tokenizer/feature granularity; etc.
      - data protocol: fix train/val split strategy; increase validation size if variance is high; ensure stratification; add seed-averaging where affordable; etc.

2) **Analyze "Model performance" (all trials)** to decide which parameters should be **finalized** (no longer searched).
   - Only use what you can infer from the trial table: parameter from performance associations across trials.
   - Identify parameters (architecture types and hyperparameters) that consistently yield top scores or have negligible impact on performance.
   - For these **finalized parameters**, lock them by using a **literal fixed value** in the Experiment constructor.
   - Examples: if `activation='gelu'` dominates, fix `activation='gelu'`. If `arch='wide_deep'` is consistently best, fix that and drop the others.

3) **Analyze "Model performance" (all trials)** to decide which parameters are still **determining** (impactful/uncertain) and should **remain in the search space**.
   - Keep only the **promising candidates** for those knobs, pruning poor performers.
   - **Do NOT just narrow** to a couple of values. Instead, construct **fine-grained local grids** centered on the best observed values:
     - Example (learning rate near 5e-4):  
       `pg.oneof([3e-4, 4e-4, 4.5e-4, 5e-4, 5.5e-4, 6e-4, 7e-4])`
     - Example (hidden size near 256–384):  
       `pg.oneof([192, 224, 256, 288, 320, 352, 384])`
   - Prefer **dense local grids** over broad coarse ranges. Expand **around** the best region from trials with a **small step size** (e.g., ±20–30% window, step by 5–10%).

4) **Increase the training sample size a bit** while respecting runtime.
   - Reduce holdout size subtly (e.g., `valid_size` from `0.2` to `0.15` or `0.10`) to train on more data.
   - Increase the sample size of the training set by 30% from the previously developed solution. If the previously developed solution used all already, do nothing.
   - Keep epochs modest to fit time budget.

Now, FILL IN THE TEMPLATE WITH:
- Finalized parameters (fixed),
- Determining parameters (still searched, pruned/tightened),
- Slightly increased training sample size via a smaller validation split.