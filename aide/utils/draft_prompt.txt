Your task is to FILL IN the provided Python Code Template as the returned markdown code block so that it becomes a fully working, self-contained NAS script using **PyGlove** and **PyTorch** and can achieve the best competition metric in the task. You must (1) analyze the competition and dataset descriptions, (2) infer the task type and metric, (3) build a symbolic search space for data processing, model architectures (including **HuggingFace Transformers**), optimizer/training strategies, and (4) train/evaluate and save a valid submission.

########### ABSOLUTE REQUIREMENTS ###########
- Frameworks: Use **PyTorch** (run on GPU via `DEVICE`). You may instantiate **Mixture-of-Experts (MoE)**, use **scikit-learn** for preprocessing, and integrate **HuggingFace Transformers** or **timm** backbones where appropriate. 
- Search Spaces (PyGlove): Define **all tunable knobs** with `pg.oneof([candidates])` (choose 1 out of N candidates), `pg.floatv(min, max)` (a float value from R[min,max]), `pg.manyof(K, [candidates], θ)` (choose K out of N candidates with optional constraints θ on the uniqueness and order of chosen candidates), or `pg.permutate([candidates])` (a special case of manyof which searches for a permutation of all candidates).  
  - Do **NOT** pass `default` to `pg.oneof` (not supported).  
  - Every symbolic choice must have concrete candidate values so the script runs immediately (no empty candidate sets).
- Keep the provided class/method signatures and the outer NAS loop/timeout helper **unchanged**.
- Decorate `Experiment` with `@pg.symbolize` and fill:
  - `__init__`: Declare symbolic knobs for **data processing**, **model family**, **optimizer**, and **training**.
  - `data_processing(...)`: Implement a **competition-aware** pipeline; expose a *symbolic* preprocessing search space. Must support BOTH:
    1) **Transformer/Text branch** (when task is text): build a tokenizer with `AutoTokenizer.from_pretrained(backbone, use_fast=True, trust_remote_code=True, token=auth_token)` and produce **dict tensors** with keys `input_ids`, `attention_mask`, and include `token_type_ids` when present; **symbolic `max_length ∈ {128, 256, 384}`**.  
    2) **Classical branch**: TF-IDF/Hashing (+ optional SVD) producing dense features; **symbolic** representation choice, ngram ranges, max_features, and optional SVD dims.
    Choose branch via a **symbolic knob** (e.g., `pg.oneof(['transformer','tfidf'])`) while still auto-detecting task type from the dataset description.
  - `model(...)`: Return a model moved to `DEVICE`. **Model family knob MUST include ALL of the latest and robust classic, Deep & Cross Network (DCN), and HF options** (at least 6 options for each). Do not use the HF Transformer released before 2023.  
    - If a Transformer is selected: load with `AutoModelForSequenceClassification.from_pretrained(backbone, num_labels=NUM_LABELS, trust_remote_code=True, token=auth_token)`.  
    - If classic: expose complex layers/widths/activations/dropouts (and MoE experts/routing). Ensure logits shape and dtype are consistent with the task.
  - `_optimizer(...)`: Choose from `pg.oneof(['Adam','AdamW','SGD','RMSprop', etc.])` as blow suggested. 
  - `training(...)`: Implement a proper loop with **symbolic** `epochs`, `batch_size` (grid sized for GPU memory, e.g., `{8, 16, 32}` for Transformers; `{64, 128, 256}` for classics), optional **gradient_accumulation_steps ∈ {1,2,4,8}**, **AMP** (`torch.cuda.amp.autocast` + `GradScaler`) and **early stopping** knobs `{enabled ∈ {True,False}, patience ∈ {2,3,4}, min_delta ∈ {0.0, 1e-4}}`.  
    - **Shape/contracts**:
      *Classic branch*: batches are `(X, y)` dense tensors.  
      *Transformer branch*: batches are `({'input_ids','attention_mask', optional 'token_type_ids'}, y)` dicts.  
    - **OOM Backoff (required)**: If CUDA OOM occurs in a Transformer run, automatically retry once with **smaller `batch_size` and/or smaller `max_length`** (halve batch_size; then reduce max_length to next lower grid value), then continue. Clear cache with `torch.cuda.empty_cache()` between retries/trials.
  - `evaluation(...)`: Compute and return the **competition metric**. For lower-is-better metrics (e.g., RMSE), return its **negative** value. 
  - `run(...)`: Build features; split train/valid (stratified for classification). Train, compute `score`, then **refit on full train** with the same hyperparameters (may skip early stop here for speed). Predict test probabilities in the correct order and `return (score, test_probs)`.
- Save a valid **submission** at `submission/submission.csv` with correct columns/format inferred from the instructions.
- **Do NOT add any new print/logging statements anywhere.** Do not log while saving the CSV.

########### COMPETITION-AWARE IMPLEMENTATION ###########
Select the relevant subset based on task type you infer from the competition text:
1) **Data Processing Search Space (`data_processing`)** Suggestions
   - For TEXT / NLP / QA / TEXT MATCHING / TOXICITY / SENTIMENT tasks:
     - Augmentations: SubwordRegularization (SentencePiece), symmetric swaps (response_a↔b TTA), Random undersampling for imbalance.
     - Features / preprocessing: rigorous TextCleaning (lowercase, URLs/HTML/punct/emoji/digits, contractions, misspellings), Tokenization (WordPiece/BPE; attention masks), Padding/Truncation, TF-IDF/Count + SVD, NB-SVM log-count ratio, length/structure counts, similarities (cosine/Jaccard/Levenshtein), OneHot/LabelEncoding for categories, distribution alignment if targets bounded.
     - QA specifics: SlidingWindow with doc_stride, span post-cleaning, char-level aggregation via offset_mapping.
     - Consider symbolic knobs but **not limit to**: representation (`pg.oneof(['tfidf_word','tfidf_char_wb','hashing_word', etc])`), ngram ranges, max_features / hashing n_features, sublinear_tf/use_idf/norm, optional SVD dims (`'none','svd256','svd512'`), stopwords, text-length features on/off, numeric side-channel scaler (`'standard','robust','none'`), optional log1p for heavy-tailed numeric features.
   - For TABULAR tasks:
     - Imbalance: SMOTE / undersampling; for ensembles use Bootstrapping.
     - Preprocessing: Imputation (mean/median/mode/KNN), Standardization/Robust/MinMax, Log/Power transforms (including target via TransformedTargetRegressor).
     - Encodings: OneHot, Label/Ordinal, TargetEncoding, frequency/count encodings; careful leakage control with CV.
     - Feature crafting: polynomial/interactions, correlation filtering, low-variance drop, PCA/SVD, adversarial validation pruning, domain distances (geo Haversine/bearings; airport/landmark distances), string parsing to structured fields.
     - Consider symbolic knobs but not limited to: Imputation knob (`'zero','median'`), scaler knob (`'standard','robust','none'`).
   - For IMAGE CLASSIFICATION (classification/detection/segmentation) tasks:
     - Augmentations to consider
       - Class imbalance or small data: RandomOversampling / Undersampling, MixUp, TTA (flip/rotate/scale).
       - Invariance to pose/orientation: Flip (H/V), Rotate/RandomRotate90, ShiftScaleRotate, Translation, Zoom/Scale, Shear, Warp/GridDistortion.
       - Robustness to lighting/color/camera: ColorJitter (brightness/contrast/saturation/hue), Gamma, CLAHE, HueSaturationValue.
       - Regularization on textures/artefacts: CutOut/CoarseDropout, Blur/NoiseInjection, JPEG artefact sim (JpegCompression).
       - Inference ensembling: TTA (flip/rot/multi-scale), Weighted Boxes Fusion for detections.
     - Feature engineering / preprocessing
       - Always: Resize to backbone input; Normalization/MinMaxScaling; Standardization to ImageNet mean/std or model-specific preprocess_input.
       - Medical/DICOM: VOILUT, HU rescaling, Photometric inversion fix, Center/ROI crop, Padding to aspect, ChannelReplication for 1→3ch, Orientation correction.
       - Gigapixel/volumes: SlidingWindow/Tiling with Overlap blending; Slice stacking or Depth pooling for 2.5D; Resampling-3D.
       - Label format needs: OneHot/MultiLabelBinarization, bbox format conversions; per-class Thresholding / small-object removal.
     - Consider symbolic knobs but not limited to: complex torchvision transforms choices (resize, crop, flip), normalize on/off; but keep light for runtime. 
   - For AUDIO tasks:
     - Augmentations: TimeShift, RandomCrop on spectrograms, NoiseInjection, SilenceTrim, WaveletDenoising, FrequencyFiltering, TTA.
     - Features: MelSpectrogram (+ dB/log), MFCC, spectral stats (centroid/rolloff/ZCR), Fixed-length padding/truncation, Standardization/MinMaxScaling, classic stats aggregation.
   - For TIME SERIES / SEQUENCES (ventilator, GNSS, EEG, volcano, OSIC) tasks:
     - Augmentations / expansions: Smoothing/KalmanSmoothing, NoiseDenoising, OutlierClipping, Intra-entity pair/window sampling.
     - Features: windowed Lag/Diff/Derivative, Rolling stats/quantiles/EWMs, Cumsum/area/integrals, interaction FeatureCrosses, GroupBy aggregates, ClusteringFeatures, Time deltas/indices, domain filters (bandpass for EEG, satellite selection for GNSS).
     - Preprocessing: Imputation (careful per entity), Robust/Standard/MinMax scaling (fit on train only), SequenceReshaping to (T, F), GroupKFold/group-aware CV to avoid leakage, postprocess to valid grids (e.g., rounding pressure to known steps).
   - For RECSYS / RANKING tasks:
     - Features: Label/TargetEncoding of IDs, DatetimeFeatures (recency, cycles), Imputation for demographics, embedding L2 normalization, co-occurrence; for ranking tasks, percentile ranks, position bias indicators.
   - For STEGANALYSIS / REMOTE SENSING / SPECIAL DOMAINS tasks:
     - Augmentations: rotations/flips; JPEG artefact simulation for steganalysis; undersampling negatives (contrails).
     - Features: false-color composites, temporal stacking/aggregation, intensity clipping, heuristic thresholding for masks.
   - Return both features `(X_all, X_test)` (and any IDs if needed inside `run()`), but keep method signature as in the template.

2) **Model Architecture Search Space (model)** Suggestions
   - For NLP / TEXT PAIRING / QA / SENTIMENT / ARENA tasks:
     - Consider but **not limit to**: use pg.oneof to select from transformers (DeBERTaV3/RemBERT/MuRIL/SentenceTransformer/LLaMA), RandomForest, LogReg, SVM, NaiveBayes, etc.

   - For IMAGE CLASSIFICATION tasks:
     - Consider but **not limit to**: use pg.oneof to select from transformers (ViT-B/L-16/32, DeiT, Swin-T), hybrid conv-attention models (CoAtNet, MaxViT/MaxxViT, MobileViT, EfficientFormer), modern CNNs (ConvNeXt/ConvNeXtV2, EfficientNetV2, NFNet, ResNet-50/101/152, ResNeSt, BiT, RexNet), MLP, tree ensembles (XGBoost, LightGBM, CatBoost, RandomForest, ExtraTrees), linear baselines (LogisticRegression, SVM), and PCA for dimensionality reduction, etc.
  
   - For TIME SERIES / SEQUENCE  tasks (e.g., tabular, sensors, bio, ventilator, GNSS, EEG):
     - Consider but **not limit to**: use pg.oneof to select from transformers (Transformer), sequence RNNs (LSTM/GRU), convolutional (CNN), graph (GNN), autoencoders (Autoencoder/Denoising AE), boosting & tree ensembles (LightGBM/XGBoost/CatBoost), linear models (Linear/ElasticNet/Ridge/Lasso), and kernel methods (SVR), etc.

   - For OBJECT DETECTION tasks:
     - Consider but **not limit to**: use pg.oneof to select from transformer detectors (DETR/YOLOS), one-stage conv detectors (YOLOv5 via torch.hub.load('ultralytics/yolov5'), EfficientDet), mobile backbones (MobileViT), and semantic segmentation (DeepLabv3), etc.

   - For RECOMMENDATION / CTR tasks:
     - Consider but **not limit to**: use pg.oneof to select from collaborative filtering (ALS), sequential recommenders (GRU4Rec/LSTM/CNN), and boosted trees for tabular/context features (gradient boosting), etc.

   - For AUDIO (spectrogram / waveform) tasks:
     - Consider but **not limit to**: use pg.oneof to select from convolutional nets (ResNet-50/101/152/CNN), sequence RNNs (LSTM/GRU), and boosting & tree ensembles, etc.

   - For RETRIEVAL/RANKING / METRICS LEARNING tasks:
     - Consider but **not limit to**: use pg.oneof to select from metric-learning architectures (Siamese Network) and classic text pipelines (TF-IDF + linear/trees stacks), etc.

   - For SPECIALIZED / OTHER tasks:
     - Consider but **not limit to**: use pg.oneof to select from unsupervised clustering (k-Means), dimensionality reduction / representation (PCA), image-to-image generative models (CycleGAN, TensorFlow), and search/meta-optimization (GeneticAlgorithm), etc.

3) **Optimizer & Training Search Space** Suggestions
   - `_optimizer`: Consider symbolic knobs but **not limit to**: use pg.oneof to select from Adam, AdamW, RMSprop, SGD, Momentum, Nesterov, Nadam, Adamax, Adagrad, Adadelta, LBFGS, SAGA, SAG, Newton-CG, liblinear, RectifiedAdam, Lookahead, MADGRAD, SLSQP, Nelder-Mead, etc.
   - `training`: implement a mini training loop over `epochs` (e.g., {4,5}) with `batch_size` choices (e.g., {128,256}), criterion choice could be:
     - Binary classification: `BCEWithLogitsLoss`.
     - Multiclass: `CrossEntropyLoss`.
     - Regression: `MSELoss` (report negative RMSE as score for “higher-is-better” convention).
   - Optional early stopping knobs: `{enabled ∈ {True,False}, patience ∈ {2,3,4}, min_delta ∈ {0.0,1e-4}}`. If enabled, monitor validation metric and restore best weights.

4) **run()**
   - Build features.
   - Stratified (if classification) hold-out split (`valid_size=0.2`) for quick feedback.
   - Train and compute validation metric via `evaluation`.
   - Refit on full train with same hyperparams (you can skip early stop here for speed).
   - Predict test set probabilities in correct order.
   - Return `(score, test_probs)`.

######################## TRANSFORMER REQUIREMENTS (MANDATORY) ########################
- **Search space must include robust HF backbones** (released after 2023) in `pg.oneof`. 
- **Tokenizer pipeline** in `data_processing` for the Transformer branch with **symbolic `max_length ∈ {128,256,384}`**; produce dict batches with `input_ids`, `attention_mask` (+ `token_type_ids` if present).
- **Training hygiene**: AMP enabled; **batch_size ∈ {8,16,32}**, **gradient_accumulation_steps ∈ {1,2,4,8}**, **AdamW** with LR `{2e-5,3e-5,5e-5}`; expose `weight_decay` and **layer-freeze ratio** knob (freeze bottom *k* encoder layers).
- Always fine-tune before inference: train the randomly initialized classifier head (and optionally top transformer layers).
- **Runtime & robustness**: Enable **early stopping**; **OOM backoff** as specified above; call `torch.cuda.empty_cache()` and `gc.collect()` between trials.
- **GPU memory**: Be mindful of GPU memory (48 GB). To avoid OOM:
  - Favor **smaller `max_length`** and **granular batch size** grids that respect memory.
  - Consider **mixed precision** (fp16/bf16), **gradient accumulation**, **mixed precision (torch.cuda.amp)**, and **activation checkpointing**.
  - Consider **chunking**: split the training data into shards/mini-epochs with smaller inputs but more training cycles (symbolic knob controlling shard count or cycles).
  - When loading HF models/tokenizers, use the authentication key (`auth_token`) in `from_pretrained(...)`.  
- When using AutoModelForSequenceClassification with a fresh head; treat it as info, not an error.
- When any DeBERTa-v3-base-mnli series model is used, it is from MoritzLaurer (i.e., `MoritzLaurer/DeBERTa-v3-base-mnli`)

########### IMPLEMENTATION TIPS TO IMPROVE CODE QUALITY (CROSS-CUTTING) ###########
- Search space should be large: each pg.oneof should offer at least 10 options.
- Separate train/val/test transforms; avoid leakage (fit scalers on train only).  
- For tokenizers/models, pass the HF auth token via `auth_token = os.getenv("HUGGINGFACE_KEY")`.  
- Prefer mixed precision for Transformers; consider activation checkpointing if easily available.  
- After each trial, free memory: delete optimizers/models/dataloaders, `torch.cuda.empty_cache()`, `gc.collect()`.

Now, FILL IN THE TEMPLATE:
- Declare all PyGlove knobs in `__init__`.
- Implement `data_processing`, `model`, `_optimizer`, `training`, `evaluation`, `run` exactly as specified above.
- Ensure `run()` returns `(score, test_probs)` and that the code runs end-to-end without modification.
