Your task is to FILL IN the provided Python Code Template as the returned markdown code block so that it becomes a fully working, self-contained NAS script using **PyGlove** and **PyTorch** and can achieve the best competition metric in the task. You must (1) analyze the competition and dataset descriptions, (2) infer the task type and metric, (3) build a symbolic search space for data processing, model architectures (including **HuggingFace Transformers**), optimizer/training strategies, and (4) train/evaluate and save a valid submission.

## ABSOLUTE REQUIREMENTS
- Frameworks: Use **PyTorch** (run on GPU via `DEVICE`). You may instantiate **Mixture-of-Experts (MoE)**, use **scikit-learn** for preprocessing, and integrate **HuggingFace Transformers** or **timm** backbones where appropriate. 
- Search Spaces (PyGlove): Define **all tunable knobs** with `pg.oneof([candidates])` (choose 1 out of N candidates), `pg.floatv(min, max)` (a float value from R[min,max]), `pg.manyof(K, [candidates], θ)` (choose K out of N candidates with optional constraints θ on the uniqueness and order of chosen candidates), or `pg.permutate([candidates])` (a special case of manyof which searches for a permutation of all candidates).  
  - Do **NOT** pass `default` to `pg.oneof` (not supported).  
  - Every symbolic choice must have concrete candidate values so the script runs immediately (no empty candidate sets).
- Keep the provided class/method signatures and the outer NAS loop/timeout helper **unchanged**.
- Decorate `Experiment` with `@pg.symbolize` and fill:
  - `__init__`: Declare symbolic knobs for **data processing**, **model family**, **optimizer**, and **training**.
  - `data_processing(...)`: Implement a **competition-aware** pipeline; expose a *symbolic* preprocessing search space. Must support BOTH:
    1) **Transformer/Text branch** (when task is text): build a tokenizer with `AutoTokenizer.from_pretrained(backbone, use_fast=True, trust_remote_code=True, token=auth_token)` and produce **dict tensors** with keys `input_ids`, `attention_mask`, and include `token_type_ids` when present; **symbolic `max_length ∈ {128, 256, 384}`**.  
    2) **Classical branch**: TF-IDF/Hashing (+ optional SVD) producing dense features; **symbolic** representation choice, ngram ranges, max_features, and optional SVD dims.
    Choose branch via a **symbolic knob** (e.g., `pg.oneof(['transformer','tfidf'])`) while still auto-detecting task type from the dataset description.
  - `model(...)`: Return a model moved to `DEVICE`. **Model family knob MUST include ALL of the latest and robust classic, Deep & Cross Network (DCN), and HF options** (at least 10 options for each). Do not use the HF Transformer released before 2023.  
    - If a Transformer is selected: load with `AutoModelForSequenceClassification.from_pretrained(backbone, num_labels=NUM_LABELS, trust_remote_code=True, token=auth_token)`.  
    - If classic: expose complex layers/widths/activations/dropouts (and MoE experts/routing). Ensure logits shape and dtype are consistent with the task.
  - `_optimizer(...)`: Choose from `pg.oneof(['Adam','AdamW','SGD','RMSprop', etc.])` as blow suggested. 
  - `training(...)`: Implement a proper loop with **symbolic** `epochs`, `batch_size` (grid sized for GPU memory, e.g., `{8, 16, 32}` for Transformers; `{64, 128, 256}` for classics), optional **gradient_accumulation_steps ∈ {1,2,4,8}**, **AMP** (`torch.cuda.amp.autocast` + `GradScaler`) and **early stopping** knobs `{enabled ∈ {True,False}, patience ∈ {2,3,4}, min_delta ∈ {0.0, 1e-4}}`.  
    - **Shape/contracts**:
      *Classic branch*: batches are `(X, y)` dense tensors.  
      *Transformer branch*: batches are `({'input_ids','attention_mask', optional 'token_type_ids'}, y)` dicts.  
    - **OOM Backoff (required)**: If CUDA OOM occurs in a Transformer run, automatically retry once with **smaller `batch_size` and/or smaller `max_length`** (halve batch_size; then reduce max_length to next lower grid value), then continue. Clear cache with `torch.cuda.empty_cache()` between retries/trials.
  - `evaluation(...)`: Compute and return the **competition metric**. For lower-is-better metrics (e.g., RMSE), return its **negative** value. 
  - `run(...)`: Build features; split train/valid (stratified for classification). Train, compute `score`, then **refit on full train** with the same hyperparameters (may skip early stop here for speed). Predict test probabilities in the correct order and `return (score, test_probs)`.
- Explicit Knob Declarations (force concrete candidate lists): You MUST declare the following search-space candidates exactly as listed below:
  - classic_arch is to list all kinds of classic architectures, e.g., mlp, RandomForest, moe, dcn, hf, where hf switches the pipeline into the Transformer branch; all other options are classic/tabular/text-classic branches. Keep "hf" in classic_arch to route the Transformer branch.
  - hf_backbone is to list at least 6 robust models from HuggingFace (post-2023 only), e.g., Qwen/Qwen2-1.5B-Instruct, intfloat/e5-small-v2, and Alibaba-NLP/gte-base-en-v1.5.
- Experiment Interface Contact (NO MISSING ATTRS):
  - Single source of truth: In Experiment.__init__, declare every attribute that may be read anywhere in the script (training, data, model, evaluation, smoke tests).
  - No ad-hoc attributes: Do not introduce new attribute names outside __init__. If you need a new flag, add it to __init__ with a default and type-appropriate PyGlove symbol. It is forbidden to access attributes not declared in __init__.
- Save a valid **submission** at `submission/submission.csv` with correct columns/format inferred from the instructions.
- **Do NOT add any new print/logging statements anywhere.** Do not log while saving the CSV.

## INPUTS YOU CAN RELY ON
- The **Task description**: describs the competition goal, task type (**infer** whether it is recommendations, classification, etc.), evaluation method, submission rules, and dataset as well as file description.
- The **Data overview**: the data tree structure which shows trainning dataset referenceing to the file description (the `Data` section).
- The **Python Code Template** (must remain structurally intact).

## COMPETITION-AWARE IMPLEMENTATION
Select the relevant subset based on task type you infer from the competition text:
1) **Data Processing Search Space (`data_processing`)** Suggestions
   - Core preprocessing
     - Imputation
       - Numeric: median imputation inside a pipeline so it's applied consistently across CV folds (imputer_numeric ∈ {median, ...}). 
       - Categorical: most-frequent (mode) imputation within the same pipeline (imputer_categorical ∈ {most_frequent, ...}).
     - Binarization
       - Map boolean/flag features to {0,1} explicitly to avoid mixed types (bool_as_int ∈ {True, False} if False, leave as boolean dtype).
     - Transaction-based feature engineering, e.g., handle entities with no recent data via a high-signal (global popularity, etc.) dummy list.
     - Standardization & sparsity-safe scaling
       - Scale numeric features with standard scaling.
       - When the design matrix is sparse (e.g., after one-hot), apply a variance scaler that **does not** center:
         - Constraint: with_mean = False on sparse matrices to preserve sparsity and avoid densification.
         - Knob: scale_sparse_onehot ∈ {True, False}
     - Category Normalization
       - Normalize semantically equivalent category labels into canonical buckets before encoding (e.g., merge near-duplicates / alias forms).
     - Categorical handling
       - One-Hot Encoding: one-hot encode categorical variables with unseen-category safety:
         - Setting: handle_unknown='ignore' to prevent inference-time errors.
   - Structured feature crafting (domain-agnostic patterns)
     - Parsing composite fields to atomic parts: split structured strings into meaningful subfields (e.g., a "compound code" into components like block/side).
     - Deriving group identifiers from IDs: extract a group key from an identifier to capture shared context among records (e.g., family/group from a passenger/customer ID prefix), e.g., group_extractor ∈ {regex_rule, delimiter_rule, none, ...}.
     - Light external inference for categorical enrichment: infer a low-cardinality attribute from an existing string token using a deterministic lookup or lightweight classifier (e.g., name → name-based attribute); keep outputs to a small, normalized category set.
     - Additive aggregations across related numerics: create sum-type rollups (e.g., related spend/count/usage components) to capture overall magnitude.
   - Pipeline hygiene
     - Ensures consistent fit/transform across CV and inference, and prevents leakage to all steps inside a single pipeline / column transformer.
   - General
     - Guarantee exactly k outputs per entity (or per row) to match leaderboard/consumer contracts. If fewer than k unique candidates are available, deterministically backfill from the global list until length k.
     - If an entity lacks recent evidence in all chosen windows, backfill from a global-popular list.
     - Construct validation exactly like the target horizon: training windows precede a held-out window with the same length and rules used for scoring.
     - Consider to use bootstrap if beneficial.

2) **Model Architecture Search Space (model)** Suggestions
   - For NLP / TEXT PAIRING / QA / SENTIMENT / ARENA tasks:
     - Consider but **not limit to**: use pg.oneof to select from transformers (DeBERTaV3/RemBERT/MuRIL/SentenceTransformer/LLaMA), RandomForest, LogReg, SVM, NaiveBayes, etc.

   - For IMAGE CLASSIFICATION tasks:
     - Consider but **not limit to**: use pg.oneof to select from transformers (ViT-B/L-16/32, DeiT, Swin-T), hybrid conv-attention models (CoAtNet, MaxViT/MaxxViT, MobileViT, EfficientFormer), modern CNNs (ConvNeXt/ConvNeXtV2, EfficientNetV2, NFNet, ResNet-50/101/152, ResNeSt, BiT, RexNet), MLP, tree ensembles (XGBoost, LightGBM, CatBoost, RandomForest), linear baselines (LogisticRegression, SVM), and PCA for dimensionality reduction, etc.
  
   - For TIME SERIES / SEQUENCE  tasks (e.g., tabular, sensors, bio, ventilator, GNSS, EEG):
     - Consider but **not limit to**: use pg.oneof to select from transformers (Transformer), sequence RNNs (LSTM/GRU), convolutional (CNN), graph (GNN), autoencoders (Autoencoder/Denoising AE), boosting & tree ensembles (LightGBM/XGBoost/CatBoost), linear models (Linear/ElasticNet/Ridge/Lasso), and kernel methods (SVR), etc.

   - For OBJECT DETECTION tasks:
     - Consider but **not limit to**: use pg.oneof to select from transformer detectors (DETR/YOLOS), one-stage conv detectors (YOLOv5 via torch.hub.load('ultralytics/yolov5'), EfficientDet), mobile backbones (MobileViT), and semantic segmentation (DeepLabv3), etc.

   - For RECOMMENDATION / CTR tasks:
     - Consider but **not limit to**: use pg.oneof to select from collaborative filtering (ALS), sequential recommenders (GRU4Rec/LSTM/CNN), and boosted trees for tabular/context features (gradient boosting), etc.

   - For AUDIO (spectrogram / waveform) tasks:
     - Consider but **not limit to**: use pg.oneof to select from convolutional nets (ResNet-50/101/152/CNN), sequence RNNs (LSTM/GRU), and boosting & tree ensembles, etc.

   - For RETRIEVAL/RANKING / METRICS LEARNING tasks:
     - Consider but **not limit to**: use pg.oneof to select from metric-learning architectures (Siamese Network) and classic text pipelines (TF-IDF + linear/trees stacks), etc.

   - For SPECIALIZED / OTHER tasks:
     - Consider but **not limit to**: use pg.oneof to select from unsupervised clustering (k-Means), dimensionality reduction / representation (PCA), image-to-image generative models (CycleGAN, TensorFlow), and search/meta-optimization (GeneticAlgorithm), etc.

3) **Optimizer & Training Search Space** Suggestions
   - `_optimizer`: Consider symbolic knobs but **not limit to**: use pg.oneof to select from Adam, AdamW, RMSprop, SGD, Momentum, Nesterov, Nadam, Adamax, Adagrad, Adadelta, LBFGS, SAGA, SAG, Newton-CG, liblinear, RectifiedAdam, Lookahead, MADGRAD, SLSQP, Nelder-Mead, etc.
   - `training`: implement a mini training loop over `epochs` (e.g., {4,5}) with `batch_size` choices (e.g., {128,256}), criterion choice could be:
     - Binary classification: `BCEWithLogitsLoss`.
     - Multiclass: `CrossEntropyLoss`.
     - Regression: `MSELoss` (report negative RMSE as score for “higher-is-better” convention).
   - Optional early stopping knobs: `{enabled ∈ {True,False}, patience ∈ {2,3,4}, min_delta ∈ {0.0,1e-4}}`. If enabled, monitor validation metric and restore best weights.

4) **smoke_test()** requirements (fast, minimal sanity run)
You MUST implement smoke_test() to run **two ultra-short** trials-one per family—to catch shallow bugs quickly without altering the main NAS loop:
   - Run matrix: Execute:
     - Exactly one HF trial using the first candidate from `hf_backbone` (respecting the tokenizer/model path and collate for dict-inputs).
     - All classic arches (i.e., iterate through every non-"hf" option listed in `classic_arch`) with the classic/dense pipeline and collate.
     - Instantiate `Experiment` by setting every declared knob declared in `__init__` in a single constructor call, choosing one valid option for each pg.oneof/manyof/floatv knob; knobs should be consistent for either the HF or classic branch.
     - Reuse the existing run_with_timeout.
   - Transformer family smoke test:
     - Construct a minimal Experiment with:
       - `classic_arch='hf'`,
       - `hf_backbone=pg.oneof([...])` but **force one** concrete pick for the smoke test (e.g., the first in the `hf_backbone` list),
       - max_length=128, batch_size_transformer=8, epochs=1, gradient_accumulation_steps=1, Automatic Mixed Precision (torch.amp) enabled, early stopping disabled.
     - Create a **tiny** sample dataset (e.g., take only the first 8 rows of train, 2 rows of valid, and 2 rows of test or synthesize two short strings with binary labels if the competition files are missing).
     - Run end-to-end: data_processing → model → training → evaluation → run() and assert:
       - The forward pass completes on DEVICE.
       - Output logits/probabilities have expected shapes.
       - Do **not** save submission CSV for the smoke test.
   - Classic family smoke test:
     - Construct a minimal Experiment with each one in classic_arch:
       - `classic_arch='dcn8'`,
       - A classic text/tabular preprocessing setting (e.g., 'tfidf_word' with a tiny max_features=1024 and SVD on),
       - batch_size_classic=64, epochs=1, early stopping disabled.
     - Use the same tiny sample dataset protocol (first few rows).
     - Run end-to-end with the same assertions as above.
   - General smoke-test rules:
     - With PyGlove `.rebind()`, a symbolic object can be manipluated into another object without modifying existing code; rebind(x, dict) is used to replace each node in x whose path is a key in dict, e.g., experiment.rebind({'classic_arch':'mlp'}) setting self.classic_arch='mlp'.
     - Use `.clone()` before using `.rebind`, e.g., `exp1 = Experiment().clone().rebind(...)`, to ensure no effect to the main branch.
     - Do not print; Do not save submission CSV; reuse the existing run_with_timeout.
     - Ensure both branches return (score, test_probs).
     - epochs = 1, batch_size minimal (e.g., 8 for HF, 64 for classic).
     - AMP on, gradient accumulation = 1.
     - No early stopping.
     - Make sure each architecture option is only run once

5) **run()**
   - Build features.
   - Stratified (if classification) hold-out split (`valid_size=0.2`) for quick feedback.
   - Train and compute validation metric via `evaluation`.
   - Refit on full train with same hyperparams (you can skip early stop here for speed).
   - Predict test set probabilities in correct order.
   - Return `(score, test_probs)`.

## TRANSFORMER REQUIREMENTS (MANDATORY)
- **Search space must include robust HF backbones** (released after 2023) in `pg.oneof`. 
- **Always fine-tune before inference**: train the classifier head or the transformer layers with matching the target classes (e.g., explicitly set num_labels=NUM_LABELS in the config and pass ignore_mismatched_sizes=True).
- **Runtime & robustness**: Enable **early stopping**; **OOM backoff** as specified above; call `torch.cuda.empty_cache()` and `gc.collect()` between trials.
- **GPU memory**: Be mindful of GPU memory (48 GB). Mush choose models wisely to avoid OOM.  


## IMPLEMENTATION TIPS TO IMPROVE CODE QUALITY (CROSS-CUTTING)
- Search space should be large: each pg.oneof should offer at least 10 options.
- Separate train/val/test transforms; avoid leakage (fit scalers on train only).  
- For tokenizers/models, pass the HF auth token via `auth_token = os.getenv("HUGGINGFACE_KEY")`.  
- Prefer mixed precision for Transformers; consider activation checkpointing if easily available.  
- When building transformer batches, pass only the fields returned by the tokenizer-do not synthesize token_type_ids if they are not provided. Ensure the **inputs you pass to model(**inputs) match the selected backbone's accepted arguments.
- If a HF backbone is chosen, build only the HF model; keep BCE shapes consistent (logits.view(-1,1) vs y.view(-1,1)) and avoid mixing classic/transformer collates.
- Enforce a single data/model branch per trial. If model_family contains HF backbone, force to use TransformerTextDataset + make_transformer_collate, batch_size_transformer, lr_transformer, etc; do not build or consume TF-IDF/Hashing/SVD/scalers, and never use ClassicTensorDataset/classic_collate. If model_family is classic, force to use dense features with ClassicTensorDataset + classic_collate, batch_size_classic, lr_classic, etc; do not instantiate HF tokenizer/model/collate or feed dict-style batches. Insert hard checks: raise if (HF ∧ classic features/collate) or (classic ∧ transformer tokenizer/collate). Maintain consistency across training/validation/test loaders.
- Diagnose and fix the transformer branch so TransformerDataset never assumes .shape on lists: normalize all tokenizer outputs to NumPy arrays (e.g., {k: np.asarray(v, dtype=np.int64)}) or PyTorch tensors before dataset creation, and compute length with len(encodings["input_ids"]).

Now, FILL IN THE TEMPLATE:
- Declare all PyGlove knobs in `__init__`.
- Implement `data_processing`, `model`, `_optimizer`, `training`, `evaluation`, `run` exactly as specified above.
- Ensure `run()` returns `(score, test_probs)` and that the code runs end-to-end without modification.
